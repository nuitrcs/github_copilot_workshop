# Module for Analyzing Scientific Abstracts
# Code generated by ChatGPT

# TODO: Finish documenting the functions

# Importing libraries

import re
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

# Defining functions

def clean_text(text, lower=True, remove_stopwords=False, verbose=False):
    """
    Cleans the input text by removing special characters and optionally lowering case and removing stopwords.

    Inputs:
        - text: The text to clean.
        - lower: Boolean flag to convert text to lowercase. Default is True.
        - remove_stopwords: Boolean flag to remove stopwords. Default is False.
        - verbose: Boolean flag to print debug messages. Default is False.

    Output:
        - cleaned_text: The cleaned text.
    """
    # Check if the input is a string
    if not isinstance(text, str):
        if verbose:
            print("clean_text -- Input must be a string.")
        return ''
    # Convert text to lowercase if the lower flag is True
    if lower:
        text = text.lower()
    # Remove special characters using regular expressions
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # Tokenize the cleaned text
    tokens = word_tokenize(text)
    # Remove stopwords if the remove_stopwords flag is True
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        tokens = [word for word in tokens if word not in stop_words]
    # Join the tokens back into a single string
    return ' '.join(tokens)

def get_word_frequencies(text, verbose=False):
    """
    Computes the frequency of each word in the given text.

    Inputs:
        - text: The text for which to calculate word frequencies.
        - verbose: Boolean flag to print debug messages. Default is False.

    Output:
        - word_freq: A dictionary with words as keys and their frequencies as values.
    """
    # Check if the input is a string
    if not isinstance(text, str):
        if verbose:
            print("get_word_frequencies -- Input must be a string.")
        return {}
    # Clean the text and tokenize it
    tokens = word_tokenize(clean_text(text))
    # Count the frequency of each word using Counter
    word_freq = Counter(tokens)
    # Return the word frequency dictionary
    return word_freq

def extract_keywords_tfidf(texts, top_n=10, verbose=False):
    if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):
        if verbose:
            print("extract_keywords_tfidf -- Input must be a list of strings.")
        return {}
    vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    keywords = {}
    for i, text in enumerate(texts):
        scores = tfidf_matrix[i].toarray()[0]
        indices = scores.argsort()[-top_n:][::-1]
        keywords[text] = [feature_names[index] for index in indices]
    return keywords

def summarize_text(text, max_words=50, verbose=False):
    if not isinstance(text, str):
        if verbose:
            print("summarize_text -- Input must be a string.")
        return ''
    tokens = word_tokenize(clean_text(text))
    return ' '.join(tokens[:max_words])

def build_wordcloud(text, save_path='wordcloud.png', verbose=False):
    from wordcloud import WordCloud
    if not isinstance(text, str):
        if verbose:
            print("build_wordcloud -- Input must be a string.")
        return
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(clean_text(text))
    wordcloud.to_file(save_path)
    if verbose:
        print(f"Word cloud saved to {save_path}.")

if __name__ == '__main__':
    print("Module with functions for analyzing scientific abstract ran as a script.")